{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3d0c34ca",
      "metadata": {
        "id": "3d0c34ca"
      },
      "source": [
        "# CS 5542 \u2014 Lab 3: Multimodal RAG Systems & Retrieval Evaluation  \n",
        "**Text + Images/PDFs (runs offline by default; optional LLM API hook)**\n",
        "\n",
        "This notebook is a **student-ready, simplified, and fully runnable** lab workflow for **multimodal retrieval-augmented generation (RAG)**:\n",
        "- ingest **PDF text** + **image captions/filenames**\n",
        "- retrieve evidence with a lightweight baseline (TF\u2011IDF)\n",
        "- build a **context block** for answering\n",
        "- evaluate retrieval quality (Precision@5, Recall@10)\n",
        "- run an **ablation study** (REQUIRED)\n",
        "\n",
        "> \u2705 **Important:** The code is optimized for **clarity + reproducibility for students** (minimal dependencies, no keys required).  \n",
        "> It is not the \u201cfastest possible\u201d or \u201cbest-performing\u201d RAG system \u2014 but it is a correct baseline that you can extend.\n",
        "\n",
        "---\n",
        "\n",
        "## Student Tasks (what you must do)\n",
        "1. **Ingest** PDFs + images from `project_data_mm/` (or use the provided sample package).  \n",
        "2. Implement / experiment with **chunking strategies** (page-based vs fixed-size).  \n",
        "3. Compare retrieval methods (at least):  \n",
        "   - **Sparse** (TF\u2011IDF / BM25-style)  \n",
        "   - **Dense** (optional: embeddings)  \n",
        "   - **Hybrid** (score fusion with `alpha`)  \n",
        "   - **Hybrid + rerank** (optional: reranker / LLM rerank)  \n",
        "4. Build a **multimodal context** that includes **evidence items** (text + images).  \n",
        "5. Produce the required **results table**:\n",
        "\n",
        "`Query \u00d7 Method \u00d7 Precision@5 \u00d7 Recall@10 \u00d7 Faithfulness`\n",
        "\n",
        "---\n",
        "\n",
        "## Expected Outputs (what graders look for)\n",
        "- Printed ingestion counts (how many PDF pages/chunks, how many images)\n",
        "- A retrieval demo showing **top\u2011k evidence** for a query\n",
        "- Evaluation metrics per method (P@5, R@10)\n",
        "- An ablation section with a small comparison table + short explanation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "734b5101",
      "metadata": {
        "id": "734b5101"
      },
      "source": [
        "## Key Parameters You Can Tune (and what they do)\n",
        "\n",
        "These parameters control retrieval + context building. **Students should change them and report what happens.**\n",
        "\n",
        "- **`TOP_K_TEXT`**: how many text chunks to consider as candidates.  \n",
        "  - Larger \u2192 more recall, but more noise (lower precision).\n",
        "- **`TOP_K_IMAGES`**: how many image items to consider as candidates.  \n",
        "  - Larger \u2192 more multimodal evidence, but can add irrelevant images.\n",
        "- **`TOP_K_EVIDENCE`**: how many total evidence items (text+image) go into the final context.  \n",
        "  - Larger \u2192 longer context; may dilute answer quality.\n",
        "- **`ALPHA`** *(0 \u2192 1)*: **fusion weight** when mixing text vs image evidence.  \n",
        "  - `ALPHA = 1.0` \u2192 text dominates  \n",
        "  - `ALPHA = 0.0` \u2192 images dominate  \n",
        "  - typical starting point: `0.5`\n",
        "- **`CHUNK_SIZE`** (fixed-size chunking): characters per chunk (baseline).  \n",
        "  - Smaller \u2192 more granular retrieval (often higher precision)  \n",
        "  - Larger \u2192 fewer chunks (often higher recall but less specific)\n",
        "- **`CHUNK_OVERLAP`**: overlap between chunks to avoid cutting important info.  \n",
        "  - Too high \u2192 redundant chunks; too low \u2192 missing context boundaries\n",
        "\n",
        "### What to try (recommended student experiments)\n",
        "- Keep everything fixed, vary **`ALPHA`**: 0.2, 0.5, 0.8  \n",
        "- Vary **`TOP_K_TEXT`**: 2, 5, 10  \n",
        "- Compare **page-based** vs **fixed-size** chunking (required ablation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fa6fe39",
      "metadata": {
        "id": "5fa6fe39"
      },
      "source": [
        "## 0) Student Info (Fill in)\n",
        "- Name: Murali Ediga\n",
        "- UMKC ID: 16334849\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "311e0454",
      "metadata": {
        "id": "311e0454"
      },
      "source": [
        "## 1) Setup (student-friendly baseline)\n",
        "\n",
        "This lab starter is designed to be **easy to run** and **easy to modify**:\n",
        "- **PyMuPDF (`fitz`)** for PDF text extraction\n",
        "- **scikit-learn** for TF\u2011IDF retrieval (strong sparse baseline)\n",
        "- **Pillow** for basic image IO\n",
        "- Optional: connect an **LLM API** for answer generation (not required to run retrieval + eval)\n",
        "\n",
        "### Student guideline\n",
        "- First make sure **retrieval + metrics** run end-to-end.\n",
        "- Then iterate: chunking \u2192 retrieval method \u2192 fusion (`ALPHA`) \u2192 rerank \u2192 faithfulness.\n",
        "\n",
        "> If you have API keys (e.g., Gemini / OpenAI / etc.), you can plug them into the optional LLM hook later \u2014  \n",
        "> but your retrieval evaluation should work **without** any external keys.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "25b3d405",
      "metadata": {
        "id": "25b3d405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 SentenceTransformers loaded.\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import os, re, glob, json, math\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# New imports for Lab 3\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
        "    HAS_DENSE = True\n",
        "    print(\"\u2705 SentenceTransformers loaded.\")\n",
        "except ImportError:\n",
        "    HAS_DENSE = False\n",
        "    print(\"\u26a0\ufe0f SentenceTransformers NOT found. Dense retrieval will be disabled.\")\n",
        "\n",
        "# Cell Description:\n",
        "# Imports all required libraries for the multimodal RAG pipeline.\n",
        "# Includes PyMuPDF for PDF processing, scikit-learn for TF-IDF, SentenceTransformers for dense retrieval,\n",
        "# and CrossEncoder for reranking. This modular setup allows the notebook to gracefully degrade\n",
        "# if optional dependencies (dense/rerank) are unavailable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d89da50c",
      "metadata": {
        "id": "d89da50c"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Lab Configuration (EDIT ME)\n",
        "# =========================\n",
        "DATA_DIR = \"project_data_mm\"   # folder containing pdfs/ and images/\n",
        "PDF_DIR  = os.path.join(DATA_DIR, \"pdfs\")\n",
        "IMG_DIR  = os.path.join(DATA_DIR, \"figures\")\n",
        "\n",
        "# Retrieval knobs\n",
        "TOP_K_TEXT     = 10   # High recall for first stage\n",
        "TOP_K_IMAGES   = 5    \n",
        "TOP_K_EVIDENCE = 8    # Final context size\n",
        "\n",
        "# Fusion knob\n",
        "ALPHA = 0.5  # 0.5 = equal weight. 1.0 = text only.\n",
        "\n",
        "# Chunking knobs\n",
        "CHUNK_SIZE    = 900\n",
        "CHUNK_OVERLAP = 150\n",
        "CHUNKING_MODE = \"fixed\" # 'page' or 'fixed'\n",
        "\n",
        "# Retrieval Method\n",
        "RETRIEVAL_METHOD = \"rerank\" # 'sparse', 'dense', 'hybrid', 'rerank'\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Cell Description:\n",
        "# Configuration parameters for the RAG pipeline. \n",
        "# We define paths, retrieval hyperparameters (k), fusion weights, and chunking settings.\n",
        "# Providing a central config block allows for easy ablation studies (changing one knob at a time).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a073bd3a",
      "metadata": {
        "id": "a073bd3a"
      },
      "source": [
        "## 2) Data folder\n",
        "Expected structure:\n",
        "```\n",
        "project_data_mm/\n",
        "  doc1.pdf\n",
        "  doc2.pdf\n",
        "  figures/\n",
        "    img1.png\n",
        "    ... (>=5)\n",
        "```\n",
        "\n",
        "If the folder is missing, we will generate **sample PDFs and images** automatically so you can run and verify the pipeline end-to-end.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "dfcc3c6d",
      "metadata": {
        "id": "dfcc3c6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 attention.pdf already exists.\n",
            "\u2705 bert.pdf already exists.\n",
            "PDFs: 2, Images: 6\n"
          ]
        }
      ],
      "source": [
        "# Data Setup - Download Papers & Extract Images\n",
        "import requests\n",
        "\n",
        "os.makedirs(PDF_DIR, exist_ok=True)\n",
        "os.makedirs(IMG_DIR, exist_ok=True)\n",
        "\n",
        "# 1. Download Research Papers (Attention + BERT)\n",
        "papers = {\n",
        "    \"attention.pdf\": \"https://arxiv.org/pdf/1706.03762.pdf\",\n",
        "    \"bert.pdf\": \"https://arxiv.org/pdf/1810.04805.pdf\"\n",
        "}\n",
        "\n",
        "for name, url in papers.items():\n",
        "    path = os.path.join(PDF_DIR, name)\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Downloading {name}...\")\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        resp = requests.get(url, headers=headers)\n",
        "        if resp.status_code == 200:\n",
        "            with open(path, \"wb\") as f:\n",
        "                f.write(resp.content)\n",
        "            print(f\"\u2705 {name} Download complete.\")\n",
        "        else:\n",
        "            print(f\"\u274c Failed to download {name}: {resp.status_code}\")\n",
        "    else:\n",
        "        print(f\"\u2705 {name} already exists.\")\n",
        "\n",
        "# 2. Extract Images (Multimodal Requirement)\n",
        "def extract_images_from_pdf(pdf_path: str, output_dir: str):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "    \n",
        "    count = 0\n",
        "    for i in range(len(doc)):\n",
        "        page = doc.load_page(i)\n",
        "        image_list = page.get_images(full=True)\n",
        "        for img_index, img in enumerate(image_list):\n",
        "            xref = img[0]\n",
        "            try:\n",
        "                base_image = doc.extract_image(xref)\n",
        "                image_bytes = base_image[\"image\"]\n",
        "                image_ext = base_image[\"ext\"]\n",
        "                # Filter tiny images\n",
        "                if len(image_bytes) < 3000: continue\n",
        "                \n",
        "                fname = f\"{base_name}_p{i+1}_{img_index+1}.{image_ext}\"\n",
        "                out_path = os.path.join(output_dir, fname)\n",
        "                with open(out_path, \"wb\") as f:\n",
        "                    f.write(image_bytes)\n",
        "                count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting image {xref}: {e}\")\n",
        "    print(f\"Extracted {count} images from {base_name}\")\n",
        "\n",
        "if len(glob.glob(os.path.join(IMG_DIR, \"*.*\"))) < 6:\n",
        "    print(\"Extracting images from PDFs...\")\n",
        "    for p in glob.glob(os.path.join(PDF_DIR, \"*.pdf\")):\n",
        "        extract_images_from_pdf(p, IMG_DIR)\n",
        "\n",
        "pdfs = sorted(glob.glob(os.path.join(PDF_DIR, \"*.pdf\")))\n",
        "imgs = sorted(glob.glob(os.path.join(IMG_DIR, \"*.*\")))\n",
        "print(f\"PDFs: {len(pdfs)}, Images: {len(imgs)}\")\n",
        "\n",
        "# Cell Description:\n",
        "# Automated ingestion script to ensure dataset requirements are met (Min 2 PDFs, Min 6 Images).\n",
        "# It downloads \"Attention Is All You Need\" and \"BERT\" papers and extracts figures using PyMuPDF.\n",
        "# This ensures the pipeline works on real-world heterogeneous PDF data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eb5e694",
      "metadata": {
        "id": "2eb5e694"
      },
      "source": [
        "## 3) Define your 3 queries + rubrics\n",
        "**Guideline:** write queries that can be answered using your PDFs/images.\n",
        "\n",
        "Rubric format below is **simple and runnable**:\n",
        "- `must_have_keywords`: words/phrases that should appear in relevant evidence\n",
        "- `optional_keywords`: nice-to-have\n",
        "\n",
        "Later, retrieval metrics will treat an evidence chunk as relevant if it contains at least one `must_have_keywords` item.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "80ccdf82",
      "metadata": {
        "id": "80ccdf82"
      },
      "outputs": [],
      "source": [
        "QUERIES = [\n",
        "    {\n",
        "        \"id\": \"Q1\",\n",
        "        \"question\": \"What is the Transformer architecture and how does it differ from RNNs/CNNs?\",\n",
        "        \"rubric\": {\n",
        "            \"must_have_keywords\": [\"transformer\", \"architecture\", \"recurrence\", \"convolution\"],\n",
        "            \"optional_keywords\": [\"attention\", \"sequential\", \"parallel\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"Q2\",\n",
        "        \"question\": \"How does BERT use the Transformer encoder?\",\n",
        "        \"rubric\": {\n",
        "            \"must_have_keywords\": [\"bert\", \"encoder\", \"bidirectional\", \"transformer\"],\n",
        "            \"optional_keywords\": [\"masked\", \"lm\", \"pre-training\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"Q3\",\n",
        "        \"question\": \"Describe the difference between the 'base' and 'large' models.\",\n",
        "        \"rubric\": {\n",
        "            \"must_have_keywords\": [\"base\", \"large\", \"parameter\", \"layer\"],\n",
        "            \"optional_keywords\": [\"dimension\", \"head\", \"performance\"]\n",
        "        }\n",
        "    },\n",
        "]\n",
        "\n",
        "# Cell Description:\n",
        "# Defines the evaluation set (Queries + Gold Standard Rubrics).\n",
        "# Q1 targets the Attention paper, Q2 targets BERT, Q3 target both/comparison.\n",
        "# The rubrics provide keywords for automated \"loose\" relevance checking in our evaluation loop.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ddd9add",
      "metadata": {
        "id": "5ddd9add"
      },
      "source": [
        "## 4) Ingestion\n",
        "We extract:\n",
        "- **PDF per-page text** as `TextChunk`\n",
        "- **Image metadata** as `ImageItem` (caption = filename without extension)\n",
        "\n",
        "> This is intentionally lightweight so it runs without downloading large embedding models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "560eb7b7",
      "metadata": {
        "id": "560eb7b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunks (fixed): 146\n",
            "Images: 6\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class TextChunk:\n",
        "    chunk_id: str\n",
        "    doc_id: str\n",
        "    page_num: int\n",
        "    text: str\n",
        "\n",
        "@dataclass\n",
        "class ImageItem:\n",
        "    item_id: str\n",
        "    path: str\n",
        "    caption: str\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    s = s or \"\"\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def extract_pdf_pages(pdf_path: str) -> List[TextChunk]:\n",
        "    doc_id = os.path.basename(pdf_path)\n",
        "    doc = fitz.open(pdf_path)\n",
        "    out: List[TextChunk] = []\n",
        "    for i in range(len(doc)):\n",
        "        page = doc.load_page(i)\n",
        "        text = clean_text(page.get_text(\"text\"))\n",
        "        if text:\n",
        "            out.append(TextChunk(\n",
        "                chunk_id=f\"{doc_id}::p{i+1}\",\n",
        "                doc_id=doc_id,\n",
        "                page_num=i+1,\n",
        "                text=text\n",
        "            ))\n",
        "    return out\n",
        "\n",
        "def chunk_text_fixed(text_chunks: List[TextChunk], chunk_size: int = 900, chunk_overlap: int = 150) -> List[TextChunk]:\n",
        "    # Refine page chunks into smaller fixed windows\n",
        "    # This allows for more precise retrieval than full-page chunks\n",
        "    new_chunks = []\n",
        "    for ch in text_chunks:\n",
        "        text = ch.text\n",
        "        if len(text) <= chunk_size:\n",
        "            new_chunks.append(ch)\n",
        "            continue\n",
        "            \n",
        "        start = 0\n",
        "        sub_idx = 0\n",
        "        while start < len(text):\n",
        "            end = min(start + chunk_size, len(text))\n",
        "            sub_text = text[start:end]\n",
        "            # Inherit metadata\n",
        "            new_chunks.append(TextChunk(\n",
        "                chunk_id=f\"{ch.chunk_id}::sub{sub_idx}\",\n",
        "                doc_id=ch.doc_id,\n",
        "                page_num=ch.page_num,\n",
        "                text=sub_text\n",
        "            ))\n",
        "            if end == len(text):\n",
        "                break\n",
        "            start += (chunk_size - chunk_overlap)\n",
        "            sub_idx += 1\n",
        "    return new_chunks\n",
        "\n",
        "def load_images(fig_dir: str) -> List[ImageItem]:\n",
        "    items: List[ImageItem] = []\n",
        "    for p in sorted(glob.glob(os.path.join(fig_dir, \"*.*\"))):\n",
        "        base = os.path.basename(p)\n",
        "        caption = os.path.splitext(base)[0].replace(\"_\", \" \")\n",
        "        items.append(ImageItem(item_id=base, path=p, caption=caption))\n",
        "    return items\n",
        "\n",
        "# Run ingestion\n",
        "raw_page_chunks = []\n",
        "for p in pdfs:\n",
        "    raw_page_chunks.extend(extract_pdf_pages(p))\n",
        "\n",
        "if CHUNKING_MODE == \"fixed\":\n",
        "    page_chunks = chunk_text_fixed(raw_page_chunks, CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "else:\n",
        "    page_chunks = raw_page_chunks\n",
        "\n",
        "image_items = load_images(IMG_DIR)\n",
        "\n",
        "print(f\"Chunks ({CHUNKING_MODE}):\", len(page_chunks))\n",
        "print(\"Images:\", len(image_items))\n",
        "\n",
        "# Cell Description:\n",
        "# Implements extraction and chunking logic.\n",
        "# 'extract_pdf_pages' gets raw text per page.\n",
        "# 'chunk_text_fixed' implements the required sliding window strategy to handle token limits \n",
        "# and focus retrieval.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf833eaf",
      "metadata": {
        "id": "cf833eaf"
      },
      "source": [
        "## 5) Retrieval (TF\u2011IDF)\n",
        "We build two TF\u2011IDF indexes:\n",
        "- One over **PDF text chunks**\n",
        "- One over **image captions**\n",
        "\n",
        "Retrieval returns the top\u2011k results with similarity scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f9fde54d",
      "metadata": {
        "id": "f9fde54d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 CrossEncoder loaded for reranking.\n",
            "\u2705 Dense index built.\n"
          ]
        }
      ],
      "source": [
        "# TF-IDF Setup\n",
        "def build_tfidf_index_text(chunks: List[TextChunk]):\n",
        "    corpus = [c.text for c in chunks]\n",
        "    vec = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
        "    X = vec.fit_transform(corpus)\n",
        "    X = normalize(X)\n",
        "    return vec, X\n",
        "\n",
        "def build_tfidf_index_images(items: List[ImageItem]):\n",
        "    corpus = [it.caption for it in items]\n",
        "    vec = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
        "    X = vec.fit_transform(corpus)\n",
        "    X = normalize(X)\n",
        "    return vec, X\n",
        "\n",
        "text_vec, text_X = build_tfidf_index_text(page_chunks)\n",
        "img_vec, img_X = build_tfidf_index_images(image_items)\n",
        "\n",
        "# Dense & Rerank Setup\n",
        "dense_model = None\n",
        "dense_embeddings = None\n",
        "reranker_model = None\n",
        "\n",
        "if HAS_DENSE:\n",
        "    # Bi-Encoder for Dense Retrieval\n",
        "    dense_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    corpus_text = [c.text for c in page_chunks]\n",
        "    dense_embeddings = dense_model.encode(corpus_text, convert_to_tensor=True)\n",
        "    \n",
        "    # Cross-Encoder for Reranking\n",
        "    try:\n",
        "        reranker_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "        print(\"\u2705 CrossEncoder loaded for reranking.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0\ufe0f CrossEncoder extract failed: {e}\")\n",
        "    \n",
        "    print(\"\u2705 Dense index built.\")\n",
        "\n",
        "def tfidf_retrieve(query: str, vec: TfidfVectorizer, X, top_k: int = 5):\n",
        "    q = vec.transform([query])\n",
        "    q = normalize(q)\n",
        "    scores = (X @ q.T).toarray().ravel()\n",
        "    idx = np.argsort(-scores)[:top_k]\n",
        "    return [(int(i), float(scores[i])) for i in idx]\n",
        "\n",
        "def dense_retrieve(query: str, top_k: int = 5):\n",
        "    if dense_model is None: return []\n",
        "    query_emb = dense_model.encode(query, convert_to_tensor=True)\n",
        "    hits = util.semantic_search(query_emb, dense_embeddings, top_k=top_k)[0]\n",
        "    return [(h['corpus_id'], h['score']) for h in hits]\n",
        "\n",
        "def rerank_hits(query: str, hit_list: List[Tuple[int, float]], top_k: int = 5):\n",
        "    if reranker_model is None or not hit_list:\n",
        "        return hit_list[:top_k]\n",
        "    \n",
        "    # Prepare pairs for cross-encoder\n",
        "    pairs = [[query, page_chunks[idx].text] for idx, score in hit_list]\n",
        "    scores = reranker_model.predict(pairs)\n",
        "    \n",
        "    # Re-sort based on new scores\n",
        "    ranked = sorted(list(zip(hit_list, scores)), key=lambda x: x[1], reverse=True)\n",
        "    # Output format is list of (idx, score)\n",
        "    return [(item[0][0], float(item[1])) for item in ranked[:top_k]]\n",
        "\n",
        "# Cell Description:\n",
        "# Initializes retrieval models.\n",
        "# - 'tfidf_retrieve': Sparse retrieval baseline using Scikit-Learn.\n",
        "# - 'dense_retrieve': Semantic search using SentenceTransformers (Bi-Encoder).\n",
        "# - 'rerank_hits': High-precision re-scoring using a Cross-Encoder (MS MARCO).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d14a7a9b",
      "metadata": {
        "id": "d14a7a9b"
      },
      "source": [
        "## 6) Build evidence context\n",
        "We assemble a compact context string + list of image paths.\n",
        "\n",
        "**Guidelines for good context:**\n",
        "- Keep snippets short (100\u2013300 chars)\n",
        "- Always include chunk IDs so you can cite evidence\n",
        "- Attach images that are likely relevant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "14f595da",
      "metadata": {
        "id": "14f595da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TEXT | attention.pdf::p3::sub0] Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively\n",
            "[IMAGE | bert_architecture.png] bert architecture\n",
            "[TEXT | attention.pdf::p10::sub1] e maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur- prisingly well, yielding b\n",
            "[TEXT | attention.pdf::p2::sub4] ks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recur\n",
            "[TEXT | attention.pdf::p2::sub2] onjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for sign\n",
            "[TEXT | attention.pdf::p1::sub2] mall fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. \u2217Equal contribution.\n",
            "[TEXT | attention.pdf::p1::sub1] st performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on \n",
            "[TEXT | attention.pdf::p10::sub2] res with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks,\n"
          ]
        }
      ],
      "source": [
        "def _normalize_scores(pairs):\n",
        "    if not pairs: return []\n",
        "    scores = [s for _, s in pairs]\n",
        "    lo, hi = min(scores), max(scores)\n",
        "    if abs(hi - lo) < 1e-12: return [(i, 1.0) for i, _ in pairs]\n",
        "    return [(i, (s - lo) / (hi - lo)) for i, s in pairs]\n",
        "\n",
        "def retrieve_text_candidates(query: str, method: str, top_k: int):\n",
        "    # Returns fused candidates\n",
        "    sparse_hits = []\n",
        "    dense_hits = []\n",
        "    \n",
        "    if method in [\"sparse\", \"hybrid\", \"rerank\"]:\n",
        "        sparse_hits = tfidf_retrieve(query, text_vec, text_X, top_k=top_k*2)\n",
        "        \n",
        "    if method in [\"dense\", \"hybrid\", \"rerank\"]:\n",
        "        dense_hits = dense_retrieve(query, top_k=top_k*2)\n",
        "        \n",
        "    if method == \"sparse\":\n",
        "        return sparse_hits[:top_k]\n",
        "    elif method == \"dense\":\n",
        "        return dense_hits[:top_k]\n",
        "    elif method == \"hybrid\":\n",
        "        # Reciprocal Rank Fusion or Linear Blend\n",
        "        combined = {}\n",
        "        s_norm = _normalize_scores(sparse_hits)\n",
        "        d_norm = _normalize_scores(dense_hits)\n",
        "        for idx, sc in s_norm: combined[idx] = combined.get(idx, 0) + (0.5 * sc)\n",
        "        for idx, sc in d_norm: combined[idx] = combined.get(idx, 0) + (0.5 * sc)\n",
        "        return sorted(combined.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "    elif method == \"rerank\":\n",
        "        # Get pool of candidates from both\n",
        "        pool_indices = set()\n",
        "        for i, s in sparse_hits: pool_indices.add(i)\n",
        "        for i, s in dense_hits: pool_indices.add(i)\n",
        "        \n",
        "        pool_list = list(pool_indices)\n",
        "        # We need scores to pass to rerank, but rerank calculates its own.\n",
        "        # Just pass them as dummy scores since CrossEncoder doesn't care about previous score.\n",
        "        candidates = [(i, 0.0) for i in pool_list]\n",
        "        return rerank_hits(query, candidates, top_k=top_k)\n",
        "    return []\n",
        "\n",
        "def build_context(\n",
        "    question: str,\n",
        "    top_k_text: int = TOP_K_TEXT,\n",
        "    top_k_images: int = TOP_K_IMAGES,\n",
        "    top_k_evidence: int = TOP_K_EVIDENCE,\n",
        "    alpha: float = ALPHA,\n",
        "    method: str = \"rerank\" \n",
        ") -> Dict[str, Any]:\n",
        "    \n",
        "    # 1. Text Retrieval\n",
        "    text_hits = retrieve_text_candidates(question, method, top_k_text)\n",
        "    \n",
        "    # 2. Image Retrieval (TF-IDF on captions)\n",
        "    img_hits = tfidf_retrieve(question, img_vec, img_X, top_k=top_k_images)\n",
        "\n",
        "    # 3. Fusion (Text vs Image)\n",
        "    # We re-normalize the ALREADY fused/retrieved text scores vs the image scores\n",
        "    text_norm = _normalize_scores(text_hits)\n",
        "    img_norm = _normalize_scores(img_hits)\n",
        "    \n",
        "    fused = []\n",
        "    for idx, s in text_norm:\n",
        "        ch = page_chunks[idx]\n",
        "        fused.append({\n",
        "            \"modality\": \"text\",\n",
        "            \"id\": ch.chunk_id,\n",
        "            \"fused_score\": alpha * s,\n",
        "            \"text\": ch.text,\n",
        "            \"path\": None\n",
        "        })\n",
        "    for idx, s in img_norm:\n",
        "        it = image_items[idx]\n",
        "        fused.append({\n",
        "            \"modality\": \"image\",\n",
        "            \"id\": it.item_id,\n",
        "            \"fused_score\": (1.0 - alpha) * s,\n",
        "            \"text\": it.caption,\n",
        "            \"path\": it.path\n",
        "        })\n",
        "        \n",
        "    fused = sorted(fused, key=lambda d: d[\"fused_score\"], reverse=True)[:top_k_evidence]\n",
        "    \n",
        "    ctx_lines = []\n",
        "    image_paths = []\n",
        "    for ev in fused:\n",
        "        if ev[\"modality\"] == \"text\":\n",
        "            snippet = (ev[\"text\"] or \"\")[:260].replace(\"\\n\", \" \")\n",
        "            ctx_lines.append(f\"[TEXT | {ev['id']}] {snippet}\")\n",
        "        else:\n",
        "            ctx_lines.append(f\"[IMAGE | {ev['id']}] {ev['text']}\")\n",
        "            image_paths.append(ev[\"path\"])\n",
        "            \n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"context\": \"\\n\".join(ctx_lines),\n",
        "        \"image_paths\": image_paths,\n",
        "        \"evidence\": fused\n",
        "    }\n",
        "\n",
        "# Demo\n",
        "print(build_context(QUERIES[0][\"question\"])[\"context\"])\n",
        "\n",
        "# Cell Description:\n",
        "# - 'retrieve_text_candidates': Central router for dispatching queries to the selected retrieval method.\n",
        "# - 'build_context': Assembles the final context window. It handles multimodal fusion (mixing text and image results using alpha weighting) to create a single coherent list of evidence for the LLM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "373612a5",
      "metadata": {
        "id": "373612a5"
      },
      "source": [
        "## 7) \u201cGenerator\u201d (simple, offline)\n",
        "To keep this notebook runnable anywhere, we implement a **lightweight extractive generator**:\n",
        "- It returns the top evidence lines\n",
        "- In your real submission, you can replace this with an LLM call (HF local model or an API)\n",
        "\n",
        "**Key rule:** the answer must stay consistent with evidence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a34c57e9",
      "metadata": {
        "id": "a34c57e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Q1 ---\n",
            "Based on evidence:\n",
            "[TEXT | attention.pdf::p3::sub0] Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively\n",
            "[IMAGE | bert_architecture.png] bert architecture\n",
            "[TEXT | attention.pdf::p10::sub1] e maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur- prisingly well, yielding b\n",
            "--- Q2 ---\n",
            "Based on evidence:\n",
            "[TEXT | bert.pdf::p3::sub3] n the pre-trained architec- ture and the \ufb01nal downstream architecture. Model Architecture BERT\u2019s model architec- ture is a multi-layer bidirectional Transformer en- coder based on the original implementation de- scribed in Vaswani et al. (2017) and released in\n",
            "[IMAGE | bert_architecture.png] bert architecture\n",
            "[IMAGE | bert_finetuning.png] bert finetuning\n",
            "--- Q3 ---\n",
            "Based on evidence:\n",
            "[TEXT | attention.pdf::p8::sub2] his model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-\n",
            "[IMAGE | attention_p3_1.png] attention p3 1\n",
            "[IMAGE | attention_p4_1.png] attention p4 1\n"
          ]
        }
      ],
      "source": [
        "def simple_extractive_answer(question: str, context: str) -> str:\n",
        "    lines = context.splitlines()\n",
        "    if not lines: return \"I don't know.\"\n",
        "    return f\"Based on evidence:\\n\" + \"\\n\".join(lines[:3])\n",
        "\n",
        "def run_query(qobj, top_k_text=TOP_K_TEXT, method=\"rerank\"):\n",
        "    q = qobj[\"question\"]\n",
        "    ctx = build_context(q, top_k_text=top_k_text, method=method)\n",
        "    ans = simple_extractive_answer(q, ctx[\"context\"])\n",
        "    return {\n",
        "        \"id\": qobj[\"id\"],\n",
        "        \"question\": q,\n",
        "        \"answer\": ans,\n",
        "        \"context\": ctx[\"context\"],\n",
        "        \"image_paths\": ctx[\"image_paths\"]\n",
        "    }\n",
        "\n",
        "for q in QUERIES:\n",
        "    res = run_query(q)\n",
        "    print(f\"--- {res['id']} ---\")\n",
        "    print(res['answer'])\n",
        "    \n",
        "# Cell Description:\n",
        "# End-to-end execution loop. It takes a query, runs retrieval, builds the context, \n",
        "# and (in this simplified version) generates an extractive answer.\n",
        "# In a full deployment, 'simple_extractive_answer' would be replaced by an LLM API call.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a4ba05a",
      "metadata": {
        "id": "9a4ba05a"
      },
      "source": [
        "## 8) Retrieval Evaluation (Precision@k / Recall@k)\n",
        "We treat a text chunk as **relevant** for a query if it contains at least one `must_have_keywords` term.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "2d16c336",
      "metadata": {
        "id": "2d16c336"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>P@5</th>\n",
              "      <th>R@10</th>\n",
              "      <th>total_relevant_chunks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Q1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.155172</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Q2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.104167</td>\n",
              "      <td>96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Q3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  P@5      R@10  total_relevant_chunks\n",
              "0  Q1  0.8  0.155172                     58\n",
              "1  Q2  1.0  0.104167                     96\n",
              "2  Q3  1.0  0.100000                    100"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def is_relevant_text(chunk_text: str, rubric: Dict[str, Any]) -> bool:\n",
        "    text = chunk_text.lower()\n",
        "    must = [k.lower() for k in rubric.get(\"must_have_keywords\", [])]\n",
        "    return any(k in text for k in must)\n",
        "\n",
        "def precision_at_k(relevances: List[bool], k: int) -> float:\n",
        "    k = min(k, len(relevances))\n",
        "    if k == 0:\n",
        "        return 0.0\n",
        "    return sum(relevances[:k]) / k\n",
        "\n",
        "def recall_at_k(relevances: List[bool], k: int, total_relevant: int) -> float:\n",
        "    k = min(k, len(relevances))\n",
        "    if total_relevant == 0:\n",
        "        return 0.0\n",
        "    return sum(relevances[:k]) / total_relevant\n",
        "\n",
        "def eval_retrieval_for_query(qobj, top_k=10) -> Dict[str, Any]:\n",
        "    question = qobj[\"question\"]\n",
        "    rubric = qobj[\"rubric\"]\n",
        "\n",
        "    hits = tfidf_retrieve(question, text_vec, text_X, top_k=top_k)\n",
        "    rels = []\n",
        "    for i, score in hits:\n",
        "        rels.append(is_relevant_text(page_chunks[i].text, rubric))\n",
        "\n",
        "    # Estimate total relevant in the corpus (for recall)\n",
        "    total_rel = sum(is_relevant_text(ch.text, rubric) for ch in page_chunks)\n",
        "\n",
        "    return {\n",
        "        \"id\": qobj[\"id\"],\n",
        "        \"P@5\": precision_at_k(rels, 5),\n",
        "        \"R@10\": recall_at_k(rels, 10, total_rel),\n",
        "        \"total_relevant_chunks\": total_rel,\n",
        "    }\n",
        "\n",
        "eval_rows = [eval_retrieval_for_query(q) for q in QUERIES]\n",
        "df_eval = pd.DataFrame(eval_rows)\n",
        "df_eval\n",
        "\n",
        "# Cell Description:\n",
        "# Implements retrieval quality metrics (Precision@K and Recall@K).\n",
        "# These metrics quantify how well our retrieval system surfaces relevant documents.\n",
        "# We use keyword-based relevance (must_have_keywords) as a lightweight ground truth,\n",
        "# which is a practical approximation for academic labs but would need human annotations in production.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de705dbf",
      "metadata": {
        "id": "de705dbf"
      },
      "source": [
        "## 9) Ablation Study (REQUIRED)\n",
        "\n",
        "You must compare **at least**:\n",
        "- **Chunking A (page-based)** vs **Chunking B (fixed-size)**  \n",
        "- **Sparse** vs **Dense** vs **Hybrid** vs **Hybrid + Rerank** *(dense/rerank can be optional extensions \u2014 but include at least sparse + one fusion variant)*  \n",
        "- **Text-only RAG** vs **Multimodal RAG** (your context must include evidence items)\n",
        "\n",
        "**Deliverable:** include a final results table in your README:\n",
        "\n",
        "`Query \u00d7 Method \u00d7 Precision@5 \u00d7 Recall@10 \u00d7 Faithfulness`\n",
        "\n",
        "### Quick ablation ideas\n",
        "- Vary `TOP_K_TEXT`: 2, 5, 10  \n",
        "- Vary `ALPHA`: 0.2, 0.5, 0.8  \n",
        "- Compare page-chunking vs fixed-size (`CHUNK_SIZE` / `CHUNK_OVERLAP`)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d8b191c1",
      "metadata": {
        "id": "d8b191c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Ablation (this may take a moment)...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Query</th>\n",
              "      <th>Method</th>\n",
              "      <th>P@5</th>\n",
              "      <th>R@10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Q1</td>\n",
              "      <td>sparse</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.155172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Q2</td>\n",
              "      <td>sparse</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.104167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Q3</td>\n",
              "      <td>sparse</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Q1</td>\n",
              "      <td>dense</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.172414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Q2</td>\n",
              "      <td>dense</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.104167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Q3</td>\n",
              "      <td>dense</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.080000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Q1</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.155172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Q2</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.104167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Q3</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.090000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Q1</td>\n",
              "      <td>rerank</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.172414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Q2</td>\n",
              "      <td>rerank</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.104167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Q3</td>\n",
              "      <td>rerank</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Query  Method  P@5      R@10\n",
              "0     Q1  sparse  0.8  0.155172\n",
              "1     Q2  sparse  1.0  0.104167\n",
              "2     Q3  sparse  1.0  0.100000\n",
              "3     Q1   dense  1.0  0.172414\n",
              "4     Q2   dense  1.0  0.104167\n",
              "5     Q3   dense  0.8  0.080000\n",
              "6     Q1  hybrid  1.0  0.155172\n",
              "7     Q2  hybrid  1.0  0.104167\n",
              "8     Q3  hybrid  1.0  0.090000\n",
              "9     Q1  rerank  1.0  0.172414\n",
              "10    Q2  rerank  1.0  0.104167\n",
              "11    Q3  rerank  1.0  0.100000"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def ablation_study():\n",
        "    methods = [\"sparse\", \"hybrid\"]\n",
        "    if HAS_DENSE: methods = [\"sparse\", \"dense\", \"hybrid\", \"rerank\"]\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    print(\"Running Ablation (this may take a moment)...\")\n",
        "    for method in methods:\n",
        "        for q in QUERIES:\n",
        "            # We reuse retrieve_text_candidates logic manually for evaluation\n",
        "            \n",
        "            top_k_text = 10\n",
        "            # Get hits using the method being tested\n",
        "            hits = retrieve_text_candidates(q[\"question\"], method, top_k_text)\n",
        "            \n",
        "            rels = []\n",
        "            for i, _ in hits:\n",
        "                rels.append(is_relevant_text(page_chunks[i].text, q[\"rubric\"]))\n",
        "            \n",
        "            # Recalculate total relevant since it depends on chunking (page_chunks global)\n",
        "            total_rel = sum(is_relevant_text(ch.text, q[\"rubric\"]) for ch in page_chunks)\n",
        "            if total_rel == 0: total_rel = 1 # Avoid div/0\n",
        "            \n",
        "            p5 = precision_at_k(rels, 5)\n",
        "            r10 = recall_at_k(rels, 10, total_rel)\n",
        "            \n",
        "            results.append({\n",
        "                \"Query\": q[\"id\"],\n",
        "                \"Method\": method,\n",
        "                \"P@5\": p5,\n",
        "                \"R@10\": r10\n",
        "            })\n",
        "            \n",
        "    df = pd.DataFrame(results)\n",
        "    return df\n",
        "\n",
        "df_ablation = ablation_study()\n",
        "df_ablation\n",
        "\n",
        "# Cell Description:\n",
        "# Systematic comparison of retrieval methods.\n",
        "# We iterate through Sparse, Dense, Hybrid, and Rerank setups, calculating Precision@5 and Recall@10\n",
        "# against our keyword-based ground truth rubrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "652c1e2d",
      "metadata": {
        "id": "652c1e2d"
      },
      "source": [
        "## 10) What to submit\n",
        "1) Your updated dataset (or keep your own)\n",
        "2) This notebook (with your answers + screenshots/outputs)\n",
        "3) A short write\u2011up: retrieval metrics + faithfulness discussion + ablation\n",
        "\n",
        "**Tip:** If you switch to an LLM, keep the same `build_context()` so the evidence is always visible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddf831bf",
      "metadata": {
        "id": "failure_analysis"
      },
      "source": [
        "## 10) Failure Analysis (REQUIRED)\n",
        "\n",
        "**Failure Case Observed:**\n",
        "For Query Q1 ('What is the Transformer architecture?'), the sparse retrieval (TF-IDF) sometimes retrieves sections about 'Training' or 'Results' because the word 'transformer' appears frequently there, missing the core 'Model Architecture' section (Section 3) if the exact keyword density is higher elsewhere.\n",
        "\n",
        "**Proposed Fix:**\n",
        "Implementing a **Reranker** (Cross-Encoder) significantly mitigated this. By taking the top-20 sparse/dense candidates and scoring them with a model trained on true relevance (MS-MARCO), the 'Model Architecture' section was consistently promoted to the top-3 because it semantically answers 'what is...', rather than just containing the keyword.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}